{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/geekculture/hugging-face-distilbert-tensorflow-for-custom-text-classification-1ad4a49e26a7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizer, TFDistilBertForSequenceClassification\n",
    "\n",
    "# default model for predefined HuggingFace sentiment-analysis pipeline.\n",
    "MODEL_NAME = 'distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the models\n",
    "\n",
    "In a real application, one might start with a pretrained model and fine-tune it for a specific application. For this example, we'll just use the pretrained model, and a pretrained tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 16:43:10.209963: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-19 16:43:10.222787: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "mytokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
    "mymodel = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize text\n",
    "\n",
    "For this example, we'll assume short texts, of less than 100 words. We'll write a function to batch encode lists of texts, and return the encoded texts as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 100\n",
    "\n",
    "# each row will be padded/truncated to exactly length MAX_LEN\n",
    "# using padding=True would pad to length of longest text (or MAX_LEN, whichever is smaller)\n",
    "def encode_texts(textlist, tokenizer=mytokenizer):\n",
    "    tokenized = tokenizer(textlist, padding='max_length', truncation=True, max_length=MAX_LEN) \n",
    "    input_array = np.array(tokenized.input_ids)\n",
    "    return input_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "textlist = [\n",
    "    'I love apples.',\n",
    "    \"But I don't like oranges.\",\n",
    "    'Papayas are weird and icky.',\n",
    "    'But mangoes are delicious.',\n",
    "    'Lilikoi, or passionfruit, is one of my favorites.',\n",
    "]\n",
    "\n",
    "encoded = encode_texts(textlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call the model to make the predictions.\n",
    "\n",
    "This model returns two outputs: the score for the negative class, and the score for the positive class, in that order.\n",
    "The scores for HuggingFace sequence classifiers are in link space, rather than probability space; in other words, the model doesn't include a final softmax layer to return probabilities. Why? I don't know why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=array([[ 0.936192  , -0.8445916 ],\n",
       "       [ 1.0879397 , -0.986854  ],\n",
       "       [ 2.2735894 , -1.9594767 ],\n",
       "       [ 0.34103048, -0.2501971 ],\n",
       "       [-0.47131115,  0.5441354 ]], dtype=float32), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = mymodel.predict(encoded)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can convert the predictions to probabilities with a softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.8557936 , 0.14420639],\n",
       "       [0.888429  , 0.11157098],\n",
       "       [0.9856996 , 0.01430037],\n",
       "       [0.6436467 , 0.35635322],\n",
       "       [0.2659153 , 0.73408467]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(output.logits).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write a \"pipeline\" function to manage the entire classification process from text to final probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_texts(texts, model=mymodel, tokenizer=mytokenizer):\n",
    "    encoded = encode_texts(texts, tokenizer=tokenizer)\n",
    "    predictions = model.predict(encoded)\n",
    "    return tf.nn.softmax(output.logits).numpy()\n",
    "\n",
    "\n",
    "# for a prettier presentation\n",
    "def classification_table(texts, predictions):\n",
    "    pframe = pd.DataFrame(predictions, columns=['prob_negative', 'prob_positive'])\n",
    "    pframe.insert(0, 'text', texts)\n",
    "    label = np.where(pframe.prob_negative > pframe.prob_positive, 'negative', 'positive')\n",
    "    pframe.insert(pframe.shape[-1], 'sentiment', label)\n",
    "    return pframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>prob_negative</th>\n",
       "      <th>prob_positive</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love apples.</td>\n",
       "      <td>0.855794</td>\n",
       "      <td>0.144206</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>But I don't like oranges.</td>\n",
       "      <td>0.888429</td>\n",
       "      <td>0.111571</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Papayas are weird and icky.</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>But mangoes are delicious.</td>\n",
       "      <td>0.643647</td>\n",
       "      <td>0.356353</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lilikoi, or passionfruit, is one of my favorites.</td>\n",
       "      <td>0.265915</td>\n",
       "      <td>0.734085</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  prob_negative  \\\n",
       "0                                     I love apples.       0.855794   \n",
       "1                          But I don't like oranges.       0.888429   \n",
       "2                        Papayas are weird and icky.       0.985700   \n",
       "3                         But mangoes are delicious.       0.643647   \n",
       "4  Lilikoi, or passionfruit, is one of my favorites.       0.265915   \n",
       "\n",
       "   prob_positive sentiment  \n",
       "0       0.144206  negative  \n",
       "1       0.111571  negative  \n",
       "2       0.014300  negative  \n",
       "3       0.356353  negative  \n",
       "4       0.734085  positive  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_table(textlist, classify_texts(textlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>prob_negative</th>\n",
       "      <th>prob_positive</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This film is so good</td>\n",
       "      <td>0.855794</td>\n",
       "      <td>0.144206</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hate this movie</td>\n",
       "      <td>0.888429</td>\n",
       "      <td>0.111571</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A great way to spend a hot summer day.</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>0.014300</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Meh. Boring</td>\n",
       "      <td>0.643647</td>\n",
       "      <td>0.356353</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'll start by saying that if you're looking fo...</td>\n",
       "      <td>0.265915</td>\n",
       "      <td>0.734085</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  prob_negative  \\\n",
       "0                               This film is so good       0.855794   \n",
       "1                                  I hate this movie       0.888429   \n",
       "2             A great way to spend a hot summer day.       0.985700   \n",
       "3                                        Meh. Boring       0.643647   \n",
       "4  I'll start by saying that if you're looking fo...       0.265915   \n",
       "\n",
       "   prob_positive sentiment  \n",
       "0       0.144206  negative  \n",
       "1       0.111571  negative  \n",
       "2       0.014300  negative  \n",
       "3       0.356353  negative  \n",
       "4       0.734085  positive  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [\"This film is so good\",\n",
    "           \"I hate this movie\",\n",
    "           \"A great way to spend a hot summer day.\",\n",
    "           \"Meh. Boring\",\n",
    "           ]\n",
    "\n",
    "# this is a positive review, but it does bring up a negative about the film (its story).\n",
    "real_review = \"I'll start by saying that if you're looking for a great story, you'll be disappointed. Shang-Chi is a pretty standard hero's journey at its core, which is a shame because the story could have been inspired by House of Flying Daggers and other wuxia titles. So why a rating so high? Because where the story falls, everything else excels.\"\n",
    "\n",
    "corpus.append(real_review)\n",
    "\n",
    "classification_table(corpus, classify_texts(corpus))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d19ba88cb96585244684ec3c8d8e8fa86134575f7079ae43c883e17be2f74d15"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('a_taste_of_data_science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

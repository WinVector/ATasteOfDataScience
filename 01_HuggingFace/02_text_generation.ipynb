{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/gpt2?text=A+long+time+ago%2C\n",
    "\n",
    "https://discuss.huggingface.co/t/how-to-decode-gpt2/16160/2 plus some tweaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 13:23:22.749435: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-05-19 13:23:22.758697: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"Hello, I'm a language model, I'm talking about the language model, not a computer model, which makes sense that, like I said,\"},\n",
       " {'generated_text': \"Hello, I'm a language model, and your first question is if this is possible or not? I don't know. Maybe this is your goal\"},\n",
       " {'generated_text': \"Hello, I'm a language model, so there's no need to write some boilerplate code, but that means that you really need nothing more than\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, a mathematical model... I\\'m gonna write something called a \"math model\" because the basic idea is. Why'},\n",
       " {'generated_text': \"Hello, I'm a language model, the model you got from this article. This is basically something I learned from my other courses by talking to people\"}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'I will go to Samarra and there Death will not find me. I shall never go there.\\n\\nIn response to this question the author stated that he knew nothing about her whereabouts, but that he did know when she was abducted, and that the authorities may seek to recover her corpse. However, he suggested that the authorities should seek to find her body for burial, and then proceed with that to the point where the person could be identified, without using any of the previous information which led to'},\n",
       " {'generated_text': 'I will go to Samarra and there Death will not find me. In all my life I have been here in Jerusalem. I shall go to a place in Jerusalem called Mount Anas and there be no life except the life of the Lord.\"\\n\\nNow, some Christians have been trying to make themselves heard, especially among children. But they have been caught in error. In Israel\\'s ancient days the Jews were called Shagruv, like our ancient fathers in the desert. So it is'},\n",
       " {'generated_text': \"I will go to Samarra and there Death will not find me. My hand will come with my name on my head. My body will go down here through the water.'\\n\\n\\nAnd in the presence of the priest he brought the image up without ceremony and the priest took it.\\n\\nAnd after the priest had been raised by him there he returned unto the temple.\\n\\nAnd it came to pass that it came to pass that I was come back which he said, after the glory of\"},\n",
       " {'generated_text': 'I will go to Samarra and there Death will not find me. And I will ask him where His body is. And he will pray it at last that it may be restored to life but when not it will be restored by His body.\"\\n\\nAnd we were in the middle of the evening of the 4th of September when this prophecy came, that the people might see that His body had been delivered from death, and after Him, which is what He came to say that His disciples brought'},\n",
       " {'generated_text': 'I will go to Samarra and there Death will not find me. All this is beyond my control. How long will it take? I will have to sleep. I will have to sleep on floor 6...\\n\\n[Saying \"This room will kill you\", to the boy, \"I shall kill you all\" to him.]\\n\\nJEDERICO CHANDEN:\\n\\n[to the girl] I had to go out and get somebody, I have no idea what to'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# \"The Appointment in Samarra\"\n",
    "# https://www.k-state.edu/english/baker/english320/Maugham-AS.htm\n",
    "generator(\n",
    "    \"\"\"I will go to Samarra and there Death will not find me.\"\"\", \n",
    "     max_length=100, \n",
    "     num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"To make a good beef stew you must first boil the vegetables, add the vinegar then add the sugar and let it all simmer for about 4 minutes, about 1 minute longer than your own. It'll look something like this: 1,742 g pork sausage\\n\\n1,742 g beef chuck\\n\\n1/4 inch salt and pepper\\n\\n1 pound minced garlic\\n\\n4 cups vegetable broth\\n\\n1/4 teaspoon ground cumin\\n\\n1 tsp cayenne pepper\\n\"},\n",
       " {'generated_text': \"To make a good beef stew you must first make it cold: the ingredients for a beef stew are a combination of salt and water, which are important to keep in stock.\\n\\nFor this recipe I cooked it in water and placed it under a cover. The cover is on the inside of the cookware so you can't peek at the sauce. I like to make the sauce that way since it will keep in the fridge for a while. Then it is ready to use.\\n\\nThen\"},\n",
       " {'generated_text': 'To make a good beef stew you must first get rid of all the excess fat from the meat and take a long, hard hard bite. If you take a hard bite, you must not add more than half of the food. You do not want to make the meat tender enough to eat over this.\\n\\nThe purpose of cooking on a high oven is not to destroy the fat but to allow the stew to get cooked. It is much better for the stew to be cooked on a high-'},\n",
       " {'generated_text': \"To make a good beef stew you must first have a good beef stew. Because if you don't, it will look and taste like nothing and no food will be prepared. Once your beef stew is done, simply chop up the onion and garlic and stir them in. I found that the potatoes did not smell so good when I poured them into the stew. The onion and garlic are the essential oils but not the purees.\\n\\n\\nIf you don't already have onions and garlic, add them\"},\n",
       " {'generated_text': \"To make a good beef stew you must first make sure that the water is clear and that it isn't going to be stirred or cooked too much.\\n\\n\\nA great stew will look very good, but at some point you may have to change the seasoning, to make it more flavorful. These are a couple of things that we usually have on hand to check for seasoning. The biggest difference is that you do what I write about, you will only need that three ingredients to make the sauce, one\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\n",
    "    \"\"\"To make a good beef stew you must first\"\"\", \n",
    "     max_length=100, \n",
    "     num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go under the covers to see the internal represntation of `generator`. Here we instantiate the tokenizer and model that the `generator` object calls, and look at the encoding and decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'task': 'text-generation',\n",
       " 'model': <transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel at 0x7f9ca9db72e0>,\n",
       " 'tokenizer': PreTrainedTokenizerFast(name_or_path='gpt2', vocab_size=50257, model_max_len=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}),\n",
       " 'feature_extractor': None,\n",
       " 'modelcard': None,\n",
       " 'framework': 'tf',\n",
       " 'device': -1,\n",
       " 'binary_output': False,\n",
       " 'call_count': 3,\n",
       " '_batch_size': None,\n",
       " '_num_workers': None,\n",
       " '_preprocess_params': {},\n",
       " '_forward_params': {},\n",
       " '_postprocess_params': {}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# peek into generator to see what the pipeline is using\n",
    "vars(generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=int32, numpy=\n",
       "array([[3041, 5372,  502,  416,  597, 2420,  345, 1549,  588,   13]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the model and tokenizer\n",
    "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# encode some input\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer.encode(text, return_tensors='tf')\n",
    "encoded_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to 50256 (first `eos_token_id`) to generate sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 20), dtype=int32, numpy=\n",
       "array([[3041, 5372,  502,  416,  597, 2420,  345, 1549,  588,   13,  198,\n",
       "         198,   40, 1101,  407, 1654,  611,  345,  821, 3910]],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run the model on the encoded input\n",
    "output = model.generate(encoded_input)  \n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Replace me by any text you'd like.\\n\\nI'm not sure if you're aware\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode the ouptut\n",
    "tokenizer.decode(output[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d19ba88cb96585244684ec3c8d8e8fa86134575f7079ae43c883e17be2f74d15"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('a_taste_of_data_science')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
